{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Quick Start: Context Engineering Dashboard\n",
    "\n",
    "Get up and running in 5 cells. This notebook shows how to:\n",
    "\n",
    "1. Build a context trace manually\n",
    "2. Visualize it in your notebook\n",
    "3. Capture a **live OpenAI call** automatically\n",
    "4. Use **ContextResource** for RAG document pools\n",
    "5. Compare two context strategies with a Sankey diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: works with pip install OR local development\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Try importing the package - if it fails, add parent directory to path\n",
    "try:\n",
    "    import context_engineering_dashboard\n",
    "    print(f\"Using installed package: {context_engineering_dashboard.__file__}\")\n",
    "except ImportError:\n",
    "    # Running from local clone - add parent directory to path\n",
    "    repo_root = Path().resolve().parent\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.insert(0, str(repo_root))\n",
    "    print(f\"Using local development: {repo_root}\")\n",
    "\n",
    "# Installation options (run one if package not found):\n",
    "# Option 1: Install from GitHub release\n",
    "# !pip install git+https://github.com/cp71-dlai/context-engineering-dashboard.git@v0.1.0\n",
    "\n",
    "# Option 2: Install locally for development (from repo root)\n",
    "# !pip install -e \".[dev,all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1 | Build a trace by hand\n",
    "\n",
    "A **ContextTrace** is the core data structure. It holds a list of\n",
    "**ContextComponents** (system prompt, user message, RAG docs, etc.)\n",
    "and the model's context-window limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_engineering_dashboard import (\n",
    "    ComponentType,\n",
    "    ContextComponent,\n",
    "    ContextTrace,\n",
    "    ContextBuilder,\n",
    ")\n",
    "\n",
    "components = [\n",
    "    ContextComponent(\"sys\",  ComponentType.SYSTEM_PROMPT, \"You are a helpful coding assistant.\", token_count=500),\n",
    "    ContextComponent(\"rag1\", ComponentType.RAG,  \"ChromaDB stores embeddings for semantic search.\", token_count=4200, metadata={\"score\": 0.93}),\n",
    "    ContextComponent(\"rag2\", ComponentType.RAG,  \"Collections group related documents together.\", token_count=2800, metadata={\"score\": 0.85}),\n",
    "    ContextComponent(\"hist\", ComponentType.CHAT_HISTORY,  \"User previously asked about installation.\", token_count=1100),\n",
    "    ContextComponent(\"user\", ComponentType.USER_MESSAGE,  \"How do I query a Chroma collection?\", token_count=350),\n",
    "]\n",
    "\n",
    "trace = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=components,\n",
    "    total_tokens=sum(c.token_count for c in components),\n",
    ")\n",
    "\n",
    "print(f\"Tokens: {trace.total_tokens:,} / {trace.context_limit:,}  ({trace.utilization:.1f}% used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## 2 | Visualize the context window\n\n`ContextBuilder` renders an interactive HTML widget right inside the notebook.\nEach colored block represents one component, sized proportionally to its token count."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the trace -- hover blocks for details, click to view content\nContextBuilder(trace=trace)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3 | Trace a live OpenAI call\n",
    "\n",
    "Wrap any `openai` call in `trace_openai()`. The tracer captures messages,\n",
    "token usage, latency, and the response -- then builds the trace for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # reads OPENAI_API_KEY from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from context_engineering_dashboard import trace_openai\n",
    "\n",
    "client = OpenAI()  # uses OPENAI_API_KEY from environment\n",
    "\n",
    "with trace_openai() as tracer:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise technical writer.\"},\n",
    "            {\"role\": \"user\",   \"content\": \"Explain what context engineering is and why it matters for LLM applications. Keep it to 3 sentences.\"},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print()\n",
    "\n",
    "openai_trace = tracer.result\n",
    "print(f\"Prompt tokens: {openai_trace.trace.usage['prompt_tokens']}\")\n",
    "print(f\"Completion tokens: {openai_trace.trace.usage['completion_tokens']}\")\n",
    "print(f\"Latency: {openai_trace.trace.latency_ms:.0f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the captured trace -- click components to view, click text to edit\n",
    "ContextBuilder(trace=openai_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4 | ContextResource for RAG document pools\n",
    "\n",
    "**ContextResource** represents a pool of items (RAG documents, examples, tools, etc.)\n",
    "that can be selected for inclusion in the context window. Use it to manage\n",
    "what content is available vs. what actually goes into the LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_engineering_dashboard import ContextResource, ResourceType\n",
    "\n",
    "# Create a resource pool from a list of documents\n",
    "rag_docs = ContextResource.from_items(\n",
    "    items=[\n",
    "        {\"id\": \"doc_1\", \"content\": \"ChromaDB is an open-source embedding database for AI applications.\", \"score\": 0.95},\n",
    "        {\"id\": \"doc_2\", \"content\": \"Collections in Chroma store documents with their embeddings.\", \"score\": 0.88},\n",
    "        {\"id\": \"doc_3\", \"content\": \"Query with collection.query(query_texts=['...'], n_results=10).\", \"score\": 0.82},\n",
    "        {\"id\": \"doc_4\", \"content\": \"Metadata filtering: use where={'field': 'value'} in queries.\", \"score\": 0.75},\n",
    "        {\"id\": \"doc_5\", \"content\": \"Chroma supports persistent storage with PersistentClient.\", \"score\": 0.70},\n",
    "    ],\n",
    "    resource_type=ResourceType.RAG,\n",
    "    name=\"Documentation\",\n",
    ")\n",
    "\n",
    "# Select the top 3 documents for inclusion\n",
    "rag_docs.select([\"doc_1\", \"doc_2\", \"doc_3\"])\n",
    "\n",
    "print(f\"Resource: {rag_docs.name}\")\n",
    "print(f\"Total items: {len(rag_docs.items)}\")\n",
    "print(f\"Selected: {len(rag_docs.selected_ids)}\")\n",
    "print(f\"Selected tokens: {rag_docs.total_selected_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert selected items to ContextComponents for the trace\n",
    "rag_components = rag_docs.to_components()\n",
    "\n",
    "for comp in rag_components:\n",
    "    print(f\"  {comp.id}: {comp.token_count} tokens, type={comp.type.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with resources panel showing available vs. selected\n",
    "# The left panel shows ALL items; the right shows what's in the context\n",
    "ContextBuilder(trace=trace, resources=[rag_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 5 | Compare two strategies with a Sankey diff\n",
    "\n",
    "Imagine you refactored a prompt: trimmed chat history and dropped a RAG doc.\n",
    "`ContextDiff` shows token flow between the two versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_engineering_dashboard import ContextDiff\n",
    "\n",
    "# \"Before\" -- verbose prompt\n",
    "before = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=[\n",
    "        ContextComponent(\"sys\",  ComponentType.SYSTEM_PROMPT, \"...\", token_count=3000),\n",
    "        ContextComponent(\"hist\", ComponentType.CHAT_HISTORY,  \"...\", token_count=18000),\n",
    "        ContextComponent(\"rag1\", ComponentType.RAG,  \"...\", token_count=12000),\n",
    "        ContextComponent(\"rag2\", ComponentType.RAG,  \"...\", token_count=5000),\n",
    "        ContextComponent(\"user\", ComponentType.USER_MESSAGE,  \"...\", token_count=400),\n",
    "    ],\n",
    "    total_tokens=38400,\n",
    ")\n",
    "\n",
    "# \"After\" -- compacted prompt (summarized history, dropped low-score doc)\n",
    "after = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=[\n",
    "        ContextComponent(\"sys\",  ComponentType.SYSTEM_PROMPT, \"...\", token_count=3000),\n",
    "        ContextComponent(\"hist\", ComponentType.CHAT_HISTORY,  \"...\", token_count=6000),\n",
    "        ContextComponent(\"rag1\", ComponentType.RAG,  \"...\", token_count=12000),\n",
    "        ContextComponent(\"user\", ComponentType.USER_MESSAGE,  \"...\", token_count=400),\n",
    "    ],\n",
    "    total_tokens=21400,\n",
    ")\n",
    "\n",
    "diff = ContextDiff(before=before, after=after, before_label=\"Verbose\", after_label=\"Compacted\")\n",
    "diff.sankey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6 | Save & reload traces\n",
    "\n",
    "Traces serialize to JSON for reproducibility and sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_trace.to_json(\"quick_start_trace.json\")\n",
    "\n",
    "reloaded = ContextTrace.from_json(\"quick_start_trace.json\")\n",
    "print(f\"Reloaded: {len(reloaded.components)} components, {reloaded.total_tokens:,} tokens\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.remove(\"quick_start_trace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next steps:** See `deep_dive.ipynb` for Chroma integration with ContextResource,\n",
    "multi-query comparisons, and advanced resource management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}