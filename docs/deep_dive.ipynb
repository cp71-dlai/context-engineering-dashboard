{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Deep Dive: Context Engineering Dashboard\n\nA comprehensive walkthrough of every feature in the dashboard.\n\n**What you will learn:**\n\n| Section | Feature |\n|---|---|\n| 1 | Chroma RAG tracing with `trace_chroma()` |\n| 2 | Available pool vs. selected context |\n| 3 | Gesture-based interactions (hover, click, double-click) |\n| 4 | Horizontal vs. Treemap layouts |\n| 5 | Live OpenAI tracing with `trace_openai()` |\n| 6 | Multi-turn conversation tracing |\n| 7 | Sankey diff for prompt optimization |\n| 8 | Context budget analysis |\n| 9 | Serialization & reproducibility |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: works with pip install OR local development\nimport sys\nfrom pathlib import Path\n\n# Try importing the package - if it fails, add parent directory to path\ntry:\n    import context_engineering_dashboard\n    print(f\"Using installed package: {context_engineering_dashboard.__file__}\")\nexcept ImportError:\n    # Running from local clone - add parent directory to path\n    repo_root = Path().resolve().parent\n    if repo_root not in sys.path:\n        sys.path.insert(0, str(repo_root))\n    print(f\"Using local development: {repo_root}\")\n\n# Installation options (run one if package not found):\n# Option 1: Install from GitHub release\n# !pip install git+https://github.com/cp71-dlai/context-engineering-dashboard.git@v0.1.0\n\n# Option 2: Install locally for development (from repo root)\n# !pip install -e \".[dev,all]\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # reads OPENAI_API_KEY from .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 | Chroma RAG tracing\n",
    "\n",
    "`trace_chroma()` wraps a Chroma collection so that every `.query()` call\n",
    "is automatically recorded -- documents, distances, scores, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"context_eng_docs\",\n",
    "    metadata={\"description\": \"Context engineering reference docs\"},\n",
    ")\n",
    "\n",
    "# Populate with realistic documentation chunks\n",
    "docs = [\n",
    "    {\n",
    "        \"id\": \"ce_overview\",\n",
    "        \"text\": (\n",
    "            \"Context engineering is the discipline of designing and optimizing the information \"\n",
    "            \"provided to a large language model within its context window. Unlike prompt engineering, \"\n",
    "            \"which focuses on instruction phrasing, context engineering considers the entire input: \"\n",
    "            \"system prompts, retrieved documents, chat history, tool outputs, and user messages. \"\n",
    "            \"The goal is to maximize the signal-to-noise ratio so the model produces accurate, \"\n",
    "            \"grounded responses.\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"overview\", \"page\": 1},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ce_components\",\n",
    "        \"text\": (\n",
    "            \"A typical LLM context window contains several component types: (1) System Prompt -- \"\n",
    "            \"defines the assistant persona and rules. (2) RAG Documents -- retrieved passages providing \"\n",
    "            \"factual grounding. (3) Chat History -- prior turns for conversational continuity. \"\n",
    "            \"(4) Tool Outputs -- results from function calls. (5) User Message -- the current query. \"\n",
    "            \"Each component competes for limited token budget.\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"components\", \"page\": 3},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ce_rag_best\",\n",
    "        \"text\": (\n",
    "            \"RAG best practices: (1) Retrieve more than you need, then re-rank and prune. \"\n",
    "            \"(2) Prefer smaller, focused chunks (200-400 tokens) over large passages. \"\n",
    "            \"(3) Include metadata (source, date, score) so the model can weigh relevance. \"\n",
    "            \"(4) Place the highest-scoring documents closest to the user query for recency bias. \"\n",
    "            \"(5) Monitor retrieval quality with similarity-score thresholds.\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"rag\", \"page\": 7},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ce_history\",\n",
    "        \"text\": (\n",
    "            \"Chat history management strategies: (1) Sliding window -- keep only the last N turns. \"\n",
    "            \"(2) Summarization -- compress older turns into a summary paragraph. \"\n",
    "            \"(3) Selective inclusion -- only include turns relevant to the current query. \"\n",
    "            \"(4) Token budgeting -- allocate a fixed percentage of the context window to history. \"\n",
    "            \"Excessive history leads to distraction and increased latency.\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"history\", \"page\": 12},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ce_tools\",\n",
    "        \"text\": (\n",
    "            \"Tool integration patterns: Function calling lets the model invoke external APIs. \"\n",
    "            \"Each tool definition consumes tokens from the context window. Best practices: \"\n",
    "            \"(1) Only include tools relevant to the current task. (2) Keep descriptions concise. \"\n",
    "            \"(3) Provide few-shot examples for complex tools. \"\n",
    "            \"(4) Monitor tool-output token counts to avoid budget overruns.\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"tools\", \"page\": 18},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ce_eval\",\n",
    "        \"text\": (\n",
    "            \"Evaluating context quality: (1) Measure answer accuracy with and without specific \"\n",
    "            \"components. (2) Track token utilization (used vs. available). (3) Compare retrieval \"\n",
    "            \"strategies using before/after diffs. (4) Log context traces for every production call \"\n",
    "            \"to identify patterns (e.g., which docs are never selected, which prompts are too long).\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"evaluation\", \"page\": 22},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"chroma_setup\",\n",
    "        \"text\": (\n",
    "            \"ChromaDB setup: pip install chromadb. Create an in-memory client with chromadb.Client() \"\n",
    "            \"or a persistent one with chromadb.PersistentClient(path='./data'). Collections store \"\n",
    "            \"documents and their embeddings. Default embedding function uses sentence-transformers.\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"chroma\", \"page\": 25},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"chroma_query\",\n",
    "        \"text\": (\n",
    "            \"Querying ChromaDB: collection.query(query_texts=['your question'], n_results=10). \"\n",
    "            \"Returns IDs, documents, distances, and metadata. Use where={'field': 'value'} for \"\n",
    "            \"metadata filtering. Distances are L2 by default; convert to similarity with 1/(1+d).\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"chroma\", \"page\": 27},\n",
    "    },\n",
    "]\n",
    "\n",
    "collection.add(\n",
    "    ids=[d[\"id\"] for d in docs],\n",
    "    documents=[d[\"text\"] for d in docs],\n",
    "    metadatas=[d[\"meta\"] for d in docs],\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection.name}' has {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_engineering_dashboard import trace_chroma, ContextWindow\n",
    "\n",
    "# Wrap the collection -- all queries are now traced\n",
    "traced = trace_chroma(collection)\n",
    "\n",
    "# Query: retrieve 6 documents (more than we will select)\n",
    "user_question = \"What are the best practices for RAG in context engineering?\"\n",
    "\n",
    "results = traced.query(\n",
    "    query_texts=[user_question],\n",
    "    n_results=6,\n",
    ")\n",
    "\n",
    "print(f\"Retrieved {len(results['ids'][0])} documents:\\n\")\n",
    "for doc_id, doc, dist in zip(\n",
    "    results[\"ids\"][0],\n",
    "    results[\"documents\"][0],\n",
    "    results[\"distances\"][0],\n",
    "):\n",
    "    score = 1.0 / (1.0 + dist)\n",
    "    print(f\"  [{doc_id}]  score={score:.3f}\")\n",
    "    print(f\"    {doc[:90]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top-3 documents for the context window\n",
    "selected_ids = results[\"ids\"][0][:3]\n",
    "traced.mark_selected(selected_ids)\n",
    "\n",
    "# Add the rest of the prompt\n",
    "traced.add_system_prompt(\n",
    "    \"You are an expert on context engineering for LLM applications. \"\n",
    "    \"Answer questions using only the provided documents. \"\n",
    "    \"Cite the source section when possible.\"\n",
    ")\n",
    "traced.add_user_message(user_question)\n",
    "\n",
    "# Build the trace\n",
    "rag_trace = traced.get_trace(context_limit=128_000)\n",
    "\n",
    "print(f\"Selected: {selected_ids}\")\n",
    "print(f\"Total tokens: {rag_trace.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the context window\n",
    "ContextWindow(trace=rag_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 | Available pool vs. selected context\n",
    "\n",
    "When `show_available_pool=True`, the dashboard shows **all** documents\n",
    "Chroma returned (left panel) next to what actually made it into the\n",
    "context window (right panel). Green = selected, grey = cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ContextWindow(trace=rag_trace, show_available_pool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the raw query trace\n",
    "query = rag_trace.chroma_queries[0]\n",
    "\n",
    "print(f\"Query: \\\"{query.query_text}\\\"\")\n",
    "print(f\"Collection: {query.collection}\")\n",
    "print(f\"Requested n_results: {query.n_results}\")\n",
    "print()\n",
    "print(f\"{'ID':<20} {'Score':>6} {'Tokens':>7} {'Status'}\")\n",
    "print(\"-\" * 50)\n",
    "for r in query.results:\n",
    "    status = \"SELECTED\" if r.selected else \"cut\"\n",
    "    print(f\"{r.id:<20} {r.score:>6.3f} {r.token_count:>7,} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3 | Gesture-based interactions\n\nThe dashboard uses intuitive mouse gestures:\n\n| Gesture | Action |\n|---------|--------|\n| **Hover** | Tooltip showing component type and token count |\n| **Click** | Opens a modal with full content and metadata |\n| **Click text in modal** | Switch to edit mode (Save button appears in header) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Try the gestures on this visualization:\n# - Hover over any block to see the tooltip\n# - Click a RAG block to see its full content and metadata\n# - Click on the text content to edit (Save button appears in header)\nContextWindow(trace=rag_trace)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 | Horizontal vs. Treemap layouts\n",
    "\n",
    "Two layout algorithms are available:\n",
    "\n",
    "- **Horizontal** (default) -- flex row where each block's width is proportional to its token count.\n",
    "  Great for comparing a few large components.\n",
    "- **Treemap** -- squarified treemap (Bruls-Huizing-van Wijk). Better when you have\n",
    "  many components of varying size, since small blocks remain visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_engineering_dashboard import ComponentType, ContextComponent, ContextTrace\n",
    "\n",
    "# Build a trace with many components to highlight layout differences\n",
    "many_components = [\n",
    "    ContextComponent(\"sys\",     ComponentType.SYSTEM_PROMPT, \"System instructions.\",     token_count=3500),\n",
    "    ContextComponent(\"rag_1\",   ComponentType.RAG_DOCUMENT,  \"Architecture overview.\",   token_count=6200),\n",
    "    ContextComponent(\"rag_2\",   ComponentType.RAG_DOCUMENT,  \"API reference.\",           token_count=4100),\n",
    "    ContextComponent(\"rag_3\",   ComponentType.RAG_DOCUMENT,  \"Deployment guide.\",        token_count=2800),\n",
    "    ContextComponent(\"hist_1\",  ComponentType.CHAT_HISTORY,  \"Turn 1.\",                  token_count=1500),\n",
    "    ContextComponent(\"hist_2\",  ComponentType.CHAT_HISTORY,  \"Turn 2.\",                  token_count=1200),\n",
    "    ContextComponent(\"tool_1\",  ComponentType.TOOL,          \"DB query result.\",         token_count=800),\n",
    "    ContextComponent(\"few_1\",   ComponentType.FEW_SHOT,      \"Example Q&A pair.\",        token_count=600),\n",
    "    ContextComponent(\"mem\",     ComponentType.MEMORY,        \"User preferences.\",        token_count=400),\n",
    "    ContextComponent(\"user\",    ComponentType.USER_MESSAGE,  \"What is the API limit?\",   token_count=250),\n",
    "]\n",
    "\n",
    "rich_trace = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=many_components,\n",
    "    total_tokens=sum(c.token_count for c in many_components),\n",
    ")\n",
    "\n",
    "print(f\"Components: {len(many_components)}, Tokens: {rich_trace.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal\n",
    "ContextWindow(trace=rich_trace, layout=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treemap -- notice how small components (Memory, Few-Shot) are easier to see\n",
    "ContextWindow(trace=rich_trace, layout=\"treemap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 | Live OpenAI tracing\n",
    "\n",
    "`trace_openai()` monkey-patches the OpenAI client to capture every\n",
    "`chat.completions.create` call inside the `with` block. Token counts\n",
    "come from the API's own usage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from context_engineering_dashboard import trace_openai\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an expert on context engineering for large language models. \"\n",
    "    \"When answering, structure your response with clear headings and examples.\"\n",
    ")\n",
    "\n",
    "with trace_openai() as tracer:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": (\n",
    "                \"Compare three chat-history management strategies for LLM context windows: \"\n",
    "                \"sliding window, summarization, and selective inclusion. \"\n",
    "                \"Include trade-offs for each.\"\n",
    "            )},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "openai_trace = tracer.result\n\nprint(f\"Model:       {openai_trace.llm_trace.model}\")\nprint(f\"Provider:    {openai_trace.llm_trace.provider}\")\nprint(f\"Prompt:      {openai_trace.llm_trace.usage.get('prompt_tokens', '?')} tokens\")\nprint(f\"Completion:  {openai_trace.llm_trace.usage.get('completion_tokens', '?')} tokens\")\nprint(f\"Latency:     {openai_trace.llm_trace.latency_ms:.0f} ms\")\nprint(f\"Context:     {openai_trace.context_limit:,} tokens\")\n\n# Click any component to see details\nContextWindow(trace=openai_trace)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 | Multi-turn conversation tracing\n",
    "\n",
    "Build a manual trace that represents a multi-turn conversation to see\n",
    "how chat history accumulates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace_openai() as tracer:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\",    \"content\": \"You are a helpful assistant. Be concise.\"},\n",
    "            {\"role\": \"user\",      \"content\": \"What is ChromaDB?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"ChromaDB is an open-source embedding database designed for AI applications. It stores and retrieves documents using vector similarity search.\"},\n",
    "            {\"role\": \"user\",      \"content\": \"How do I add documents to a collection?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Use collection.add(ids=['id1'], documents=['text'], metadatas=[{'key': 'val'}]). IDs must be unique strings.\"},\n",
    "            {\"role\": \"user\",      \"content\": \"Now show me how to query with metadata filtering.\"},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content[:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "multi_turn_trace = tracer.result\n\nprint(f\"Components: {len(multi_turn_trace.components)}\")\nfor c in multi_turn_trace.components:\n    print(f\"  {c.id:<15} {c.type.value:<15} {c.token_count:>5} tokens\")\n\n# Click components to see chat history content\nContextWindow(trace=multi_turn_trace)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 | Sankey diff for prompt optimization\n",
    "\n",
    "`ContextDiff` renders an SVG Sankey diagram showing how token allocation\n",
    "changed between two versions of a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_engineering_dashboard import ContextDiff\n",
    "\n",
    "# Scenario: optimize a production RAG prompt\n",
    "before = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=[\n",
    "        ContextComponent(\"sys\",    ComponentType.SYSTEM_PROMPT, \"...\", token_count=5000),\n",
    "        ContextComponent(\"rag_1\",  ComponentType.RAG_DOCUMENT,  \"...\", token_count=8000),\n",
    "        ContextComponent(\"rag_2\",  ComponentType.RAG_DOCUMENT,  \"...\", token_count=6000),\n",
    "        ContextComponent(\"rag_3\",  ComponentType.RAG_DOCUMENT,  \"...\", token_count=4000),\n",
    "        ContextComponent(\"hist\",   ComponentType.CHAT_HISTORY,  \"...\", token_count=22000),\n",
    "        ContextComponent(\"tools\",  ComponentType.TOOL,          \"...\", token_count=3000),\n",
    "        ContextComponent(\"user\",   ComponentType.USER_MESSAGE,  \"...\", token_count=500),\n",
    "    ],\n",
    "    total_tokens=48500,\n",
    ")\n",
    "\n",
    "# After optimization:\n",
    "# - Summarized chat history (22K -> 5K)\n",
    "# - Dropped lowest-scoring RAG doc\n",
    "# - Trimmed system prompt\n",
    "after = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=[\n",
    "        ContextComponent(\"sys\",    ComponentType.SYSTEM_PROMPT, \"...\", token_count=2500),\n",
    "        ContextComponent(\"rag_1\",  ComponentType.RAG_DOCUMENT,  \"...\", token_count=8000),\n",
    "        ContextComponent(\"rag_2\",  ComponentType.RAG_DOCUMENT,  \"...\", token_count=6000),\n",
    "        ContextComponent(\"hist\",   ComponentType.CHAT_HISTORY,  \"...\", token_count=5000),\n",
    "        ContextComponent(\"tools\",  ComponentType.TOOL,          \"...\", token_count=3000),\n",
    "        ContextComponent(\"user\",   ComponentType.USER_MESSAGE,  \"...\", token_count=500),\n",
    "    ],\n",
    "    total_tokens=25000,\n",
    ")\n",
    "\n",
    "diff = ContextDiff(\n",
    "    before=before,\n",
    "    after=after,\n",
    "    before_label=\"v1 (Verbose)\",\n",
    "    after_label=\"v2 (Optimized)\",\n",
    ")\n",
    "\n",
    "diff.sankey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side by side: before and after\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<h3>BEFORE</h3>\"))\n",
    "display(ContextWindow(trace=before))\n",
    "display(HTML(\"<h3>AFTER</h3>\"))\n",
    "display(ContextWindow(trace=after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8 | Context budget analysis\n",
    "\n",
    "Programmatically inspect where your tokens go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the RAG trace from Section 1\n",
    "trace = rag_trace\n",
    "\n",
    "print(\"CONTEXT BUDGET ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Context limit:  {trace.context_limit:>10,} tokens\")\n",
    "print(f\"Tokens used:    {trace.total_tokens:>10,} tokens\")\n",
    "print(f\"Tokens free:    {trace.unused_tokens:>10,} tokens\")\n",
    "print(f\"Utilization:    {trace.utilization:>9.1f}%\")\n",
    "print()\n",
    "print(f\"{'Component Type':<22} {'Tokens':>8} {'% of Used':>10} {'Count':>6}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for comp_type in ComponentType:\n",
    "    comps = trace.get_components_by_type(comp_type)\n",
    "    if comps:\n",
    "        total = sum(c.token_count for c in comps)\n",
    "        pct = (total / trace.total_tokens) * 100 if trace.total_tokens else 0\n",
    "        print(f\"  {comp_type.value:<20} {total:>8,} {pct:>9.1f}% {len(comps):>5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual component breakdown\n",
    "print(f\"\\n{'ID':<20} {'Type':<18} {'Tokens':>7}\")\n",
    "print(\"-\" * 50)\n",
    "for c in trace.components:\n",
    "    print(f\"{c.id:<20} {c.type.value:<18} {c.token_count:>7,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9 | Serialization & reproducibility\n",
    "\n",
    "Traces serialize to JSON so you can save snapshots, share with teammates,\n",
    "and compare across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save to JSON\n",
    "rag_trace.to_json(\"deep_dive_trace.json\")\n",
    "print(\"Saved trace to deep_dive_trace.json\")\n",
    "\n",
    "# Peek at the JSON structure\n",
    "with open(\"deep_dive_trace.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"\\nJSON keys: {list(data.keys())}\")\n",
    "print(f\"Components: {len(data['components'])}\")\n",
    "print(f\"Chroma queries: {len(data.get('chroma_queries', []))}\")\n",
    "print(f\"Total tokens: {data['total_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reload and visualize\nreloaded = ContextTrace.from_json(\"deep_dive_trace.json\")\n\nprint(f\"Reloaded: {len(reloaded.components)} components, {reloaded.total_tokens:,} tokens\")\nprint(f\"Chroma queries: {len(reloaded.chroma_queries)}\")\n\nContextWindow(trace=reloaded, show_available_pool=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dict for programmatic access\n",
    "trace_dict = rag_trace.to_dict()\n",
    "\n",
    "# Example: extract all RAG scores\n",
    "for comp in trace_dict[\"components\"]:\n",
    "    if comp[\"type\"] == \"rag_document\" and comp.get(\"metadata\"):\n",
    "        score = comp[\"metadata\"].get(\"chroma_score\", \"N/A\")\n",
    "        print(f\"  {comp['id']}: score={score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "os.remove(\"deep_dive_trace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n| Feature | How to use it |\n|---|---|\n| Chroma tracing | `traced = trace_chroma(collection)` then `.query()` / `.mark_selected()` |\n| OpenAI tracing | `with trace_openai() as t:` wraps any `client.chat.completions.create` call |\n| Visualization | `ContextWindow(trace=trace, layout=\"horizontal\")` |\n| Interactions | Hover (tooltip), Click (modal), Click text (edit) |\n| Available pool | `ContextWindow(trace=trace, show_available_pool=True)` |\n| Diff view | `ContextDiff(before, after).sankey()` |\n| Save/load | `trace.to_json('file.json')` / `ContextTrace.from_json('file.json')` |\n| Budget analysis | `trace.utilization`, `trace.unused_tokens`, `trace.get_components_by_type()` |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}