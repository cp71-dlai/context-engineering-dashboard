{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Context Engineering Dashboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "```bash\n",
    "# install from Github \n",
    "\n",
    "pip install git+https://github.com/cp71-dlai/context-engineering-dashboard.git@v0.2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbbb5a-deb0-4b4c-a4f7-0aba5323b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context engineering dashboard \n",
    "\n",
    "from context_engineering_dashboard import (\n",
    "    ComponentType,\n",
    "    ContextComponent,\n",
    "    ContextTrace,\n",
    "    ContextBuilder,\n",
    "    ContextResource, \n",
    "    ResourceType,\n",
    "    ContextDiff\n",
    ")\n",
    "\n",
    "# 3rd party integrations\n",
    "\n",
    "from context_engineering_dashboard import (\n",
    "    trace_openai, \n",
    "    trace_litellm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads from .env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1 | Build a trace by hand\n",
    "\n",
    "A **ContextTrace** is the core data structure. It holds a list of\n",
    "**ContextComponents** (system prompt, user message, RAG docs, etc.)\n",
    "and the model's context-window limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_engineering_dashboard import (\n",
    "    ComponentType,\n",
    "    ContextComponent,\n",
    "    ContextTrace,\n",
    "    ContextBuilder,\n",
    ")\n",
    "\n",
    "components = [\n",
    "    ContextComponent(\"sys\",  ComponentType.SYSTEM_PROMPT, \"You are a helpful coding assistant.\", token_count=500),\n",
    "    ContextComponent(\"rag1\", ComponentType.RAG,  \"ChromaDB stores embeddings for semantic search.\", token_count=4200, metadata={\"score\": 0.93}),\n",
    "    ContextComponent(\"rag2\", ComponentType.RAG,  \"Collections group related documents together.\", token_count=2800, metadata={\"score\": 0.85}),\n",
    "    ContextComponent(\"hist\", ComponentType.CHAT_HISTORY,  \"User previously asked about installation.\", token_count=1100),\n",
    "    ContextComponent(\"user\", ComponentType.USER_MESSAGE,  \"How do I query a Chroma collection?\", token_count=350),\n",
    "]\n",
    "\n",
    "trace = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=components,\n",
    "    total_tokens=sum(c.token_count for c in components),\n",
    ")\n",
    "\n",
    "print(f\"Tokens: {trace.total_tokens:,} / {trace.context_limit:,}  ({trace.utilization:.1f}% used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2 | Visualize the context window\n",
    "\n",
    "`ContextBuilder` renders an interactive HTML widget right inside the notebook.\n",
    "Each colored block represents one component, sized proportionally to its token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trace -- hover blocks for details, click to view content\n",
    "ContextBuilder(trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3 | Trace OpenAI call\n",
    "\n",
    "Wrap any `openai` call in `trace_openai()`. The tracer captures messages,\n",
    "token usage, latency, and the response -- then builds the trace for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()  # uses OPENAI_API_KEY from environment\n",
    "\n",
    "with trace_openai() as tracer:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise technical writer.\"},\n",
    "            {\"role\": \"user\",   \"content\": \"Explain what context engineering is and why it matters for LLM applications. Keep it to 3 sentences.\"},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print()\n",
    "\n",
    "openai_trace = tracer.result\n",
    "print(f\"Prompt tokens: {openai_trace.trace.usage['prompt_tokens']}\")\n",
    "print(f\"Completion tokens: {openai_trace.trace.usage['completion_tokens']}\")\n",
    "print(f\"Latency: {openai_trace.trace.latency_ms:.0f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the captured trace -- click components to view, click text to edit\n",
    "ContextBuilder(trace=openai_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4b322-5941-44e3-a6c3-bc477845c88e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 | Trace a LiteLLM Call\n",
    "\n",
    "LiteLLM provides a unified API for 100+ LLM providers. The `trace_litellm()` \n",
    "tracer captures calls regardless of which backend you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e93bc-6c42-4d39-b113-b66c1f4bcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "# Using OpenAI via LiteLLM (bare model name defaults to OpenAI)\n",
    "with trace_litellm() as tracer:\n",
    "    response = litellm.completion(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert on context engineering.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What are the key components of an LLM context window? Answer in 2-3 sentences.\"},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9fd385-c4c8-4889-b5d7-2105e78b7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the captured trace\n",
    "litellm_trace = tracer.result\n",
    "\n",
    "print(f\"Provider:    {litellm_trace.trace.provider}\")\n",
    "print(f\"Model:       {litellm_trace.trace.model}\")\n",
    "print(f\"Prompt:      {litellm_trace.trace.usage.get('prompt_tokens', '?')} tokens\")\n",
    "print(f\"Completion:  {litellm_trace.trace.usage.get('completion_tokens', '?')} tokens\")\n",
    "print(f\"Latency:     {litellm_trace.trace.latency_ms:.0f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09849553-f020-4029-b32a-fdb3bbb0c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the context window\n",
    "ContextBuilder(trace=litellm_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5ba61-b552-4946-9377-5c22f4f5e2e2",
   "metadata": {},
   "source": [
    "LiteLLM uses `provider/model-name` format to route to different backends:\n",
    "\n",
    "| Model String | Provider |\n",
    "|---|---|\n",
    "| `gpt-4o` | OpenAI (default) |\n",
    "| `anthropic/claude-3-opus` | Anthropic |\n",
    "| `azure/gpt-4` | Azure OpenAI |\n",
    "| `bedrock/anthropic.claude-v2` | AWS Bedrock |\n",
    "| `gemini/gemini-pro` | Google |\n",
    "\n",
    "The tracer automatically extracts the provider name for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5 | Build a resource by hand\n",
    "\n",
    "**ContextResource** represents a pool of items (RAG documents, examples, tools, etc.)\n",
    "that can be selected for inclusion in the context window. Use it to manage\n",
    "what content is available vs. what actually goes into the LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fecff-3796-4b59-9df5-794156a48089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trace\n",
    "\n",
    "components = [\n",
    "    ContextComponent(\"sys\",  ComponentType.SYSTEM_PROMPT, \"You are a helpful coding assistant.\", token_count=500),\n",
    "    ContextComponent(\"hist\", ComponentType.CHAT_HISTORY,  \"User previously asked about installation.\", token_count=1100),\n",
    "    ContextComponent(\"user\", ComponentType.USER_MESSAGE,  \"How do I query a Chroma collection?\", token_count=350),\n",
    "]\n",
    "\n",
    "trace = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=components,\n",
    "    total_tokens=sum(c.token_count for c in components),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a resource pool from a list of documents\n",
    "rag_docs = ContextResource.from_items(\n",
    "    items=[\n",
    "        {\"id\": \"doc_1\", \"content\": \"ChromaDB is an open-source embedding database for AI applications.\", \"score\": 0.95},\n",
    "        {\"id\": \"doc_2\", \"content\": \"Collections in Chroma store documents with their embeddings.\", \"score\": 0.88},\n",
    "        {\"id\": \"doc_3\", \"content\": \"Query with collection.query(query_texts=['...'], n_results=10).\", \"score\": 0.82},\n",
    "        {\"id\": \"doc_4\", \"content\": \"Metadata filtering: use where={'field': 'value'} in queries.\", \"score\": 0.75},\n",
    "        {\"id\": \"doc_5\", \"content\": \"Chroma supports persistent storage with PersistentClient.\", \"score\": 0.70},\n",
    "    ],\n",
    "    resource_type=ResourceType.RAG,\n",
    "    name=\"Documentation\",\n",
    ")\n",
    "\n",
    "# Select the top 3 documents for inclusion\n",
    "rag_docs.select([\"doc_1\", \"doc_2\", \"doc_3\"])\n",
    "\n",
    "print(f\"Resource: {rag_docs.name}\")\n",
    "print(f\"Total items: {len(rag_docs.items)}\")\n",
    "print(f\"Selected: {len(rag_docs.selected_ids)}\")\n",
    "print(f\"Selected tokens: {rag_docs.total_selected_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert selected items to ContextComponents for the trace\n",
    "rag_components = rag_docs.to_components()\n",
    "\n",
    "for comp in rag_components:\n",
    "    print(f\"  {comp.id}: {comp.token_count} tokens, type={comp.type.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with resources panel showing available vs. selected\n",
    "# The left panel shows ALL items; the right shows what's in the context\n",
    "ContextBuilder(trace=trace, resources=[rag_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16e84c-3487-4723-8969-1ca770bf9c35",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 | Chroma Integration\n",
    "\n",
    "`ContextResource.from_chroma()` wraps a Chroma collection so you can query it\n",
    "and manage document selection for your context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fea98-8804-4cd0-85c1-1cae7741fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"context_eng_docs\",\n",
    "    metadata={\"description\": \"Context engineering reference docs\"},\n",
    ")\n",
    "\n",
    "# Populate with realistic documentation chunks\n",
    "doc_data = [\n",
    "    {\n",
    "        \"id\": \"ce_overview\",\n",
    "        \"text\": (\n",
    "            \"Context engineering is the discipline of designing and optimizing the information \"\n",
    "            \"provided to a large language model within its context window. Unlike prompt engineering, \"\n",
    "            \"which focuses on instruction phrasing, context engineering considers the entire input.\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"overview\", \"page\": 1},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ce_rag_best\",\n",
    "        \"text\": (\n",
    "            \"RAG best practices: (1) Retrieve more than you need, then re-rank and prune. \"\n",
    "            \"(2) Prefer smaller, focused chunks (200-400 tokens) over large passages. \"\n",
    "            \"(3) Include metadata (source, date, score) so the model can weigh relevance.\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"rag\", \"page\": 7},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ce_tools\",\n",
    "        \"text\": (\n",
    "            \"Tool integration patterns: Function calling lets the model invoke external APIs. \"\n",
    "            \"Each tool definition consumes tokens from the context window. Best practices: \"\n",
    "            \"Only include tools relevant to the current task. Keep descriptions concise.\"\n",
    "        ),\n",
    "        \"meta\": {\"section\": \"tools\", \"page\": 18},\n",
    "    },\n",
    "]\n",
    "\n",
    "collection.add(\n",
    "    ids=[d[\"id\"] for d in doc_data],\n",
    "    documents=[d[\"text\"] for d in doc_data],\n",
    "    metadatas=[d[\"meta\"] for d in doc_data],\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection.name}' has {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52d962-8afe-4535-b3d1-624624b3b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ContextResource from a Chroma collection\n",
    "rag_resource = ContextResource.from_chroma(\n",
    "    collection=collection,\n",
    "    resource_type=ResourceType.RAG,\n",
    "    name=\"Documentation\",\n",
    ")\n",
    "\n",
    "# Query the resource (queries the underlying Chroma collection)\n",
    "user_question = \"What are the best practices for RAG?\"\n",
    "\n",
    "rag_resource.query(\n",
    "    query_texts=[user_question],\n",
    "    n_results=3,\n",
    ")\n",
    "\n",
    "print(f\"Query: '{user_question}'\\n\")\n",
    "print(f\"Retrieved {len(rag_resource.items)} documents:\")\n",
    "for item in rag_resource.items:\n",
    "    print(f\"  [{item.id}] score={item.score:.3f}, {item.token_count} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c4361-ac5d-425b-9bd8-5069dfea00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 2 documents and visualize with resource pool\n",
    "top_ids = [item.id for item in rag_resource.items[:2]]\n",
    "rag_resource.select(top_ids)\n",
    "\n",
    "# Build a trace with the selected documents\n",
    "chroma_trace = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=[\n",
    "        ContextComponent(\"sys\", ComponentType.SYSTEM_PROMPT, \"You are helpful.\", token_count=5),\n",
    "        ContextComponent(\"user\", ComponentType.USER_MESSAGE, user_question, token_count=10),\n",
    "    ] + rag_resource.to_components(),\n",
    "    total_tokens=15 + rag_resource.total_selected_tokens,\n",
    ")\n",
    "\n",
    "# Show available pool (left) vs context (right)\n",
    "ContextBuilder(trace=chroma_trace, resources=[rag_resource])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6 | Explore effect of compaction with a Sankey diff\n",
    "\n",
    "Imagine you refactored a prompt: trimmed chat history and dropped a RAG doc.\n",
    "`ContextDiff` shows token flow between the two versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Before\" -- verbose prompt\n",
    "before = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=[\n",
    "        ContextComponent(\"sys\",  ComponentType.SYSTEM_PROMPT, \"...\", token_count=3000),\n",
    "        ContextComponent(\"hist\", ComponentType.CHAT_HISTORY,  \"...\", token_count=18000),\n",
    "        ContextComponent(\"rag1\", ComponentType.RAG,  \"...\", token_count=12000),\n",
    "        ContextComponent(\"rag2\", ComponentType.RAG,  \"...\", token_count=5000),\n",
    "        ContextComponent(\"user\", ComponentType.USER_MESSAGE,  \"...\", token_count=400),\n",
    "    ],\n",
    "    total_tokens=38400,\n",
    ")\n",
    "\n",
    "# \"After\" -- compacted prompt (summarized history, dropped low-score doc)\n",
    "after = ContextTrace(\n",
    "    context_limit=128_000,\n",
    "    components=[\n",
    "        ContextComponent(\"sys\",  ComponentType.SYSTEM_PROMPT, \"...\", token_count=3000),\n",
    "        ContextComponent(\"hist\", ComponentType.CHAT_HISTORY,  \"...\", token_count=6000),\n",
    "        ContextComponent(\"rag1\", ComponentType.RAG,  \"...\", token_count=12000),\n",
    "        ContextComponent(\"user\", ComponentType.USER_MESSAGE,  \"...\", token_count=400),\n",
    "    ],\n",
    "    total_tokens=21400,\n",
    ")\n",
    "\n",
    "diff = ContextDiff(before=before, after=after, before_label=\"Verbose\", after_label=\"Compacted\")\n",
    "diff.sankey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6 | Save & reload traces\n",
    "\n",
    "Traces serialize to JSON for reproducibility and sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_trace.to_json(\"quick_start_trace.json\")\n",
    "\n",
    "reloaded = ContextTrace.from_json(\"quick_start_trace.json\")\n",
    "print(f\"Reloaded: {len(reloaded.components)} components, {reloaded.total_tokens:,} tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
